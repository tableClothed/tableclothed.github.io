<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      AWS Machine Learning Specialty Bite Size Recap 2/3 &middot; portfolio - weronika zak
    
  </title>


<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" integrity="sha384-JcKb8q3iqJ61gNV9KGb8thSsNjpSL0n8PARn9HuZOnIxN0hoP+VmmDGMN5t9UJ0Z" crossorigin="anonymous">

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,400italic|Source+Code+Pro:400,700" type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.min.css" type="text/css">
  <link rel="stylesheet" href="/css/style.css" type="text/css">

  <link rel="icon" type="image/png" sizes="192x192" href="/favicon.png">
  <link rel="shortcut icon" href="/favicon.ico">
  <link rel="apple-touch-icon-precomposed" sizes="152x152" href="/apple-touch-icon.png">
  
  <link rel="alternate" type="application/atom+xml" title="portfolio - weronika zak" href="/atom.xml">

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/fontawesome.min.css" integrity="sha384-jLKHWM3JRmfMU0A5x5AkjWkw/EYfGUAGagvnfryNV3F9VqM98XiIH7VBGVoxVSc7" crossorigin="anonymous">


</head>


  <body>
    <nav class="nav-main">
      <ul>
        <li class="hvr-underline-reveal"><a href="/pages/about/">about me</a></li>
        <li class="logo"><a class="hvr-ripple-out" href="/">W</a></li>
        <li class="hvr-underline-reveal"><a href="/pages/blog/">my blog</a></li>
      </ul>
    </nav>

    <div class="container content">
      <main>
        <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
</script>

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<article class="page">
    <header>
        <h1 class="landing-title">
            
            
            


                
                AWS Machine Learning Specialty
            <span style="color: #e6e6e6;">
                Bite Size Recap 2/3
            </span>
        </h1>
    </header>

  <!-- <center>
<img src="/images/posts/Other/aws/aws_ml.png">
</center> -->

<h2 id="aws-machine-learning-specialty-23">AWS Machine Learning Specialty 2/3</h2>

<h3 id="model-retraining">Model Retraining</h3>
<ul>
  <li>Retraining with a larger number of epochs doesn’t make sense if the model has already reached the global minimum on test data.</li>
</ul>

<h3 id="dropout-regularization">Dropout Regularization</h3>
<ul>
  <li>Applying dropout regularization at the flatten layer is incorrect.</li>
  <li>Dropout is typically used to combat overfitting, and its application depends on the gap between validation error and training error.</li>
</ul>

<h3 id="model-complexity">Model Complexity</h3>
<ul>
  <li>Augmenting model complexity by increasing the number of layers is incorrect.</li>
  <li>Increasing layers may negatively impact the model, potentially causing overfitting.</li>
</ul>

<h3 id="aws-glue-data-catalog">AWS Glue Data Catalog</h3>
<ul>
  <li>Contains references to data used in ETL jobs.</li>
  <li>Essential for creating data warehouses or data lakes.</li>
  <li>Serves as an index for location, schema, and runtime metrics.</li>
  <li>Information stored as metadata tables.</li>
</ul>

<h3 id="emr-cluster-vs-aws-glue">EMR Cluster vs. AWS Glue</h3>
<ul>
  <li>Creating an EMR cluster involves more configuration effort than AWS Glue.</li>
</ul>

<h3 id="aws-data-pipeline-and-aws-glue-data-catalog">AWS Data Pipeline and AWS Glue Data Catalog</h3>
<ul>
  <li>Using AWS Data Pipeline to automate data transformation jobs and AWS Glue Data Catalog for storing metadata is incorrect.</li>
  <li>Requires configuring and managing compute resources for EMR.</li>
</ul>

<h3 id="amazon-emr">Amazon EMR</h3>
<ul>
  <li>Instantly provisions capacity for data-intensive tasks.</li>
  <li>Suitable for applications like web indexing, data mining, log file analysis, machine learning, and more.</li>
  <li>Eliminates the need for time-consuming setup, management, or tuning of clusters.</li>
</ul>

<h3 id="amazon-quicksight">Amazon QuickSight</h3>
<ul>
  <li>Scalable, serverless, embeddable BI service.</li>
  <li>Machine learning-powered business intelligence for the cloud.</li>
  <li>Enables easy creation and publication of interactive BI dashboards with ML-powered insights.</li>
</ul>

<h3 id="generating-precision-recall-data">Generating Precision-Recall Data</h3>
<ul>
  <li>Amazon EMR is the best choice for generating precision-recall data, especially for big data processing (150TB).</li>
</ul>

<h3 id="custom-cloudwatch-dashboards">Custom CloudWatch Dashboards</h3>
<ul>
  <li>Direct creation of custom CloudWatch dashboards from S3 data is not possible.</li>
</ul>

<h3 id="redshift-in-the-scenario">Redshift in the Scenario</h3>
<ul>
  <li>Redshift has no application in this scenario; it is only used to store the output of EMR.</li>
</ul>

<h3 id="pipe-input-mode-vs-file-input-mode-in-sagemaker">Pipe Input Mode vs. File Input Mode in SageMaker</h3>
<ul>
  <li><strong>Pipe Input Mode</strong></li>
  <li>Data fed on-the-fly into the algorithm container without involving disk I/O.</li>
  <li>Shortens download process and reduces startup time.</li>
  <li>Generally better read throughput than File input mode.</li>
  <li>Enables training on datasets larger than the 16 TB EBS volume size limit.</li>
  <li><strong>File Input Mode</strong></li>
  <li>Default mode for training in Amazon SageMaker.</li>
  <li>Increases throughput but not the best choice among the given options.</li>
</ul>

<h3 id="amazon-elastic-inference">Amazon Elastic Inference</h3>
<ul>
  <li>Allows attaching low-cost GPU-powered acceleration to EC2, Sagemaker, or ECS tasks.</li>
  <li>Reduces deep learning inference costs by up to 75%.</li>
  <li>Supports TensorFlow, Apache MXNet, PyTorch, and ONNX models.</li>
  <li>Enables precise configuration of GPU-powered inference acceleration.</li>
</ul>

<h3 id="text-cleaning-in-nlp">Text Cleaning in NLP</h3>
<ul>
  <li>Integral stage in NLP pipeline for structured processing of unstructured texts.</li>
  <li>Examples include lowercase conversion, word tokenization, stop word removal, HTML tag removal, stemming, lemmatization, etc.</li>
</ul>

<h3 id="fixing-spelling-errors">Fixing Spelling Errors</h3>
<ul>
  <li>Correcting a specific word (“niht” to “night”) is impractical for all posts.</li>
</ul>

<h3 id="part-of-speech-pos-tagging">Part-of-Speech (PoS) Tagging</h3>
<ul>
  <li>Primarily used for categorizing words in a text corpus, not for text preprocessing.</li>
</ul>

<h3 id="one-hot-encoding-vs-word2vec">One-Hot Encoding vs. Word2Vec</h3>
<ul>
  <li>One-hot encoding is unsuitable for Word2Vec as it poorly captures semantics between words.</li>
  <li>Tokenization is a better approach for processing individual words.</li>
</ul>

<h3 id="sagemaker-object2vec-algorithm-components">SageMaker Object2Vec Algorithm Components</h3>
<ul>
  <li>Two input channels, two encoders (enc0 and enc1), and a comparator.</li>
  <li>Comparator compares embeddings and outputs scores indicating relationship strength.</li>
  <li>Encoders convert objects into fixed-length embedding vectors for comparison.</li>
  <li>Dropout hyperparameter reduces overfitting by trimming codependent neurons.</li>
  <li>L1 regularization is not available for Amazon SageMaker Object2Vec; it’s used for simple regression models.</li>
</ul>

<p>Happy learning! :)</p>

</article>

      </main>

      <footer class="footer">
        <small>
            <span class="copyright"><i class="fa fa-copyright"></i> <time datetime="2024-01-25T22:23:56+00:00">2024</time> Weronika Zak</span> &middot;
            <span>Powered by <a class="link-white-highlight" href="http://jekyllrb.com/">Jekyll</a></span>
        </small>
        <div class="ftr-links">
          <a class="link-white-highlight" href="https://github.com/weronikazak"><i class="fa fa-github-alt"></i></a>
        </div>
      </footer>
    </div>

  </body>
</html>
