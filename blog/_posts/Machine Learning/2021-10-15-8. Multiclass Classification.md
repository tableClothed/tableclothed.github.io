---
layout: post
title:  "Multiclass Classification"
date:   2021-10-16 16:10:35 +0100
tags:
- machine learning
description: "Multiclass classification is approached by expanding the definition of y to include more than two categories. This problem is divided into n+1 binary classification problems, and binary logistic regression is applied to each case to predict which class the data belongs to. The hypothesis that returns the highest value is used as the prediction."
image: 'images/posts/Machine Learning/ml8/0.jpg'

---

<center>
<img src="/images/posts/Machine Learning/ml8/0.jpg">
</center>

## Multiclass Classification: One-vs-All

Now, we will approach the classification of data when we have more than two categories. Instead of $y = \{ 0, 1 \}$ we will expand our definition so that $y = \{ 0, 1 ... n \}$.

Since $y = \{ 0, 1 ... n \}$, we divide our problem into $n + 1$ ($+ 1$ because the index starts at 0) binary classification problems; in each one, we predict the probability that $y$ is a member of one of our classes.

<div class="latex-eq">
$$
\begin{align}
& y \in \{ 0, 1, ... n \} \\
& h_{\theta}^{(0)}(x) = P(y = 0 | x; 0) \\
& h_{\theta}^{(1)}(x) = P(y = 1 | x; 0) \\
& ... \\
& h_{\theta}^{(n)}(x) = P(y = n | x; 0) \\
& \text{prediction} = \text{max}(h_{\theta}^{(i)}(x))
\end{align}
$$
</div>

We are basically choosing one class and then lumping all the others into a single second class. We do this repeatedly, applying binary logistic regression to each case, and then use the hypothesis that returned the highest value as our prediction.

The following image shows how one could classify 3 classes:

<center>
<img src="/images/posts/Machine Learning/ml8/1.png">
</center>

#### To summarize:

Train a logic regresion classifier $h_{\theta}(x)$ for each class to predict the probability that $y = i$.

To make a prediction on a new $x$, pick the class that maximizes $h_{\theta}(x)$.


### THE NOTES WERE BASED ON <a class="link-white-highlight" href="https://www.coursera.org/learn/machine-learning"> THIS COURSE BY ANDREW NG</a>.